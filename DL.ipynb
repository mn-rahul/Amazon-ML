{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mn-rahul/Amazon-ML/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hs5Gc4J-lyZ"
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM, Dropout, Activation, Embedding, Bidirectional\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bYmuFCv-xYF",
        "outputId": "80445fae-ee3c-4b3c-87ae-2ed51958314e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho6fGUjL-vOy"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/drive/MyDrive/AmazonML/balancedtrain.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "SzOGMTZ2_KyA",
        "outputId": "aa8f2cf0-d54c-4676-a856-cc350166bd61"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pete the cat bedtim blue doll 14 5 inch pete t...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the new yorker nyhm014 refriger magnet 2 x 3 5...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the ultim self suffici handbook a complet guid...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amway nutrilit kid chewabl iron tablet 100 nan...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>teacher planner compani a4 6 lesson academ tea...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439440</th>\n",
              "      <td>kimtech wettask system 30902 meltblownbas shee...</td>\n",
              "      <td>23593.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439441</th>\n",
              "      <td>marvain wifi wireless display dongl wireless h...</td>\n",
              "      <td>196786.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439442</th>\n",
              "      <td>l squar woodwork tool aluminium alloy right an...</td>\n",
              "      <td>42101.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439443</th>\n",
              "      <td>the nxt lvl liquid silicon soft back cover cas...</td>\n",
              "      <td>794481.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439444</th>\n",
              "      <td>torr to face shield pack of 5 with adjust elas...</td>\n",
              "      <td>1044933.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>439445 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        x          y\n",
              "0       pete the cat bedtim blue doll 14 5 inch pete t...        0.0\n",
              "1       the new yorker nyhm014 refriger magnet 2 x 3 5...        1.0\n",
              "2       the ultim self suffici handbook a complet guid...        2.0\n",
              "3       amway nutrilit kid chewabl iron tablet 100 nan...        3.0\n",
              "4       teacher planner compani a4 6 lesson academ tea...        4.0\n",
              "...                                                   ...        ...\n",
              "439440  kimtech wettask system 30902 meltblownbas shee...    23593.0\n",
              "439441  marvain wifi wireless display dongl wireless h...   196786.0\n",
              "439442  l squar woodwork tool aluminium alloy right an...    42101.0\n",
              "439443  the nxt lvl liquid silicon soft back cover cas...   794481.0\n",
              "439444  torr to face shield pack of 5 with adjust elas...  1044933.0\n",
              "\n",
              "[439445 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6bfXOGi_MkC",
        "outputId": "dbc12323-082d-475b-c1fd-7efdba2a8656"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaHn98-u_Szn"
      },
      "source": [
        "x=train['x'].astype('str')\n",
        "y=train['y']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D48ajcQYGtbB",
        "outputId": "f57c2a0e-4e09-40fd-d92a-c76ef4670738"
      },
      "source": [
        "classes = y.nunique()\n",
        "classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9919"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8thsUuf_bY3"
      },
      "source": [
        "# vocab_size = 5000 # make the top list of words (common words)\n",
        "# embedding_dim = 64\n",
        "# max_length = 200b\n",
        "# trunc_type = 'post'\n",
        "# padding_type = 'post'\n",
        "# oov_tok = '<OOV>' # OOV = Out of Vocabulary\n",
        "# training_portion = .8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyHJJS0Qam-i"
      },
      "source": [
        "max_features = 20000\n",
        "max_text_length = 400 #find the text with max length in code and change it for each column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tevFWnMFcDQF"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam#Try all of these\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "durN5gsDan4I",
        "outputId": "74400acc-bd07-445f-db1b-a1c1a8ed0c30"
      },
      "source": [
        "x_tokenizer =text.Tokenizer(max_features)\n",
        "x_tokenizer.fit_on_texts(list(x))\n",
        "print(\"1/2\")\n",
        "x_tokenized = x_tokenizer.texts_to_sequences(x)\n",
        "print(\"2/2\")\n",
        "x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Happened\n",
            "Happened\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2sEYNLzatOZ",
        "outputId": "9c1a7c5e-cdd5-4903-b610-1fb30a197048"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-01 08:49:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-01 08:49:59--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-01 08:49:59--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zip.1â€™\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.15MB/s    in 2m 40s  \n",
            "\n",
            "2021-08-01 08:52:40 (5.13 MB/s) - â€˜glove.6B.zip.1â€™ saved [862182613/862182613]\n",
            "\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Hv2oEZa5LG",
        "outputId": "4d005e73-936f-4293-ed57-d623cb8e5ab7"
      },
      "source": [
        "embedding_dim = 300\n",
        "embedding_index = dict()\n",
        "f = open('glove.6B.300d.txt', encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embedding_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embedding_index))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_Qbkn4ha-mJ"
      },
      "source": [
        "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
        "for word, index in x_tokenizer.word_index.items():\n",
        "  if index>max_features -1:\n",
        "    break\n",
        "  else:\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector\n",
        "del(embedding_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQT4X7-ZpP_S",
        "outputId": "8c91bf9b-35ce-4846-ba4b-968c3f7f7c27"
      },
      "source": [
        "word_index = x_tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 271744 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kkecHvobBuK"
      },
      "source": [
        "embedding_matrix=np.float32(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-qvGLGTbDRK"
      },
      "source": [
        "import pickle\n",
        "dbfile = open('embeddingmatrix.pkl','ab')\n",
        "pickle.dump(embedding_matrix, dbfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBJ0sKTJbEkr"
      },
      "source": [
        "embedding_dim = 300\n",
        "max_features = 20000\n",
        "max_text_length = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYKl8sMmbHRF"
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(x_train_val,\n",
        "                                                  y,\n",
        "                                                  random_state=1,\n",
        "                                                  test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zmrzsym98w-",
        "outputId": "dcbb1cf4-86d6-49aa-8698-c1f10bf276fd"
      },
      "source": [
        "x_train.shape,y_train.shape,x_val.shape,y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((373528, 400), (373528,), (65917, 400), (65917,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "Iv6C6PlO9tzM",
        "outputId": "6473aa5d-9c1a-4516-c54e-c8732e5be0c4"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(x_train, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1777c5375aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [373528, 65917]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If9fdNyYbJVY"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Embedding(max_features,\n",
        "#                     embedding_dim,\n",
        "#                     embeddings_initializer=tf.keras.initializers.Constant(\n",
        "#                         embedding_matrix)))#,trainable=False\n",
        "# model.add(Dropout(0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEd-J0s6bLUp"
      },
      "source": [
        "# no_of_classes=len(y.unique())\n",
        "# no_of_classes\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3FfZcUMbNu6"
      },
      "source": [
        "# model.add(Conv1D(500,\n",
        "#                  3,\n",
        "#                  padding='valid'))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dense(12000, activation='tanh'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(no_of_classes,activation='softmax'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwbXQg-hbZw8"
      },
      "source": [
        "# model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05iA2VYtbcNJ"
      },
      "source": [
        "# batch_size = 128\n",
        "# epochs = 3\n",
        "# hist_adam=model.fit(x_train, y_train,\n",
        "#           batch_size=batch_size,\n",
        "#           epochs=epochs,\n",
        "#           validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhQH6v9PobjB"
      },
      "source": [
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.layers import Dense, Input, Flatten\n",
        "# from keras.layers import Reshape, Dropout, Concatenate\n",
        "# from keras.layers import Conv2D, MaxPool2D, Embedding\n",
        "# from keras.models import Model\n",
        "# from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
        "# from keras.layers.embeddings import Embedding\n",
        "# from keras.models import Model\n",
        "# import string\n",
        "# import re\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from sklearn.preprocessing import LabelBinarizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# import keras\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras.layers import SpatialDropout1D\n",
        "# from keras.layers import CuDNNLSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXk7W7TKpHa_"
      },
      "source": [
        "# model2 = Sequential()\n",
        "# model2.add(Embedding(max_features,embedding_dim,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)))\n",
        "# model2.add(SpatialDropout1D(0.2))\n",
        "# model2.add(Bidirectional(CuDNNLSTM(64, return_sequences=True)))\n",
        "# model2.add(Bidirectional(CuDNNLSTM(32)))\n",
        "# model2.add(Dropout(0.25))\n",
        "# model2.add(Dense(units=5, activation='softmax'))\n",
        "# model2.compile(loss = 'sparse_categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "# print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlk4cniwb2Dj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp38oUlPq6sh"
      },
      "source": [
        "# model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k6-fv9RoX0d"
      },
      "source": [
        "# model2.fit(x_train, y_train, batch_size=64, epochs=5,validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIPfgxc3qwuo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}